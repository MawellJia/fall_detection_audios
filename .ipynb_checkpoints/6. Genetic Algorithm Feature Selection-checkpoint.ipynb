{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Feature Selection\n",
    "\n",
    "### This file has the feature selection process based on Genetic Algorithm.\n",
    "\n",
    "1. Load the extracted features from the spreadsheet.\n",
    "2. The parameters for fitness iterations are declared.\n",
    "3. The binary converted input is sent to the iteration.\n",
    "4. The input is checked for the strength of the features.\n",
    "5. Repeat for maximum iterations.\n",
    "6. The fitness is compared and the most fit feature is selected.\n",
    "7. Most fit features are chosen for parent selection.\n",
    "8. The parents selected are subjected to mating to enhance feature quality.\n",
    "9. Crossover and mutation counts and their rates are recorded as well.\n",
    "10. The results of crossover and mutations are merged.\n",
    "11. The fitness if enhanced is recorded, stored and updated.\n",
    "12. The best feature subset dictionary is created.\n",
    "13.  Repeat for other selected parents.\n",
    "14.  Plot the accuracy vs crossover graph and loss vs mutation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-gJyh-ATgZ9"
   },
   "source": [
    "Display the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.25024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.258743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.771681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174262</td>\n",
       "      <td>0.466904</td>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.083640</td>\n",
       "      <td>4.186918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025914</td>\n",
       "      <td>0.722178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100532</td>\n",
       "      <td>1.279070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268812</td>\n",
       "      <td>0.291504</td>\n",
       "      <td>0.151343</td>\n",
       "      <td>0.947338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.371120</td>\n",
       "      <td>2.076422</td>\n",
       "      <td>4.462774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.417882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.322780</td>\n",
       "      <td>0.262438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306021</td>\n",
       "      <td>1.423760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231787</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.15868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1    2         3    4          5         6         7         8         9  \\\n",
       "0  0.0  0.0  37.25024  0.0   1.258743  0.000000  0.000000  0.000000  0.004778   \n",
       "1  0.0  0.0   0.00000  0.0  19.083640  4.186918  0.000000  0.025914  0.722178   \n",
       "2  0.0  0.0   0.00000  0.0  25.371120  2.076422  4.462774  0.000000  0.000000   \n",
       "3  0.0  0.0   0.00000  0.0  23.322780  0.262438  0.000000  0.189533  0.000000   \n",
       "4  0.0  0.0  14.15868  0.0   0.000000  0.015535  0.000000  0.000000  0.000000   \n",
       "\n",
       "         10  ...       125       126       127       128       129       130  \\\n",
       "0  0.771681  ...  0.000000  0.286896  0.000000  0.174262  0.466904  0.425832   \n",
       "1  0.000000  ...  0.100532  1.279070  0.000000  0.268812  0.291504  0.151343   \n",
       "2  0.000000  ...  0.000000  0.000000  0.920719  0.000000  1.417882  0.000000   \n",
       "3  0.606273  ...  0.000000  0.000000  0.000000  0.000000  0.306021  1.423760   \n",
       "4  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.360398   \n",
       "\n",
       "        131  132       133  Labels  \n",
       "0  0.000000  0.0  0.220118       0  \n",
       "1  0.947338  0.0  0.082670       0  \n",
       "2  0.000000  0.0  0.000000       0  \n",
       "3  0.000000  0.0  0.231787       0  \n",
       "4  0.000000  0.0  0.000000       0  \n",
       "\n",
       "[5 rows x 134 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Kannan-PC\\Desktop\\MS\\Projects\\fall detection audios\\Audio_Features.csv')\n",
    "df.dataframeName = 'Audio_Features.csv'\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define methods needed to perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Zl00FjJATgOy"
   },
   "outputs": [],
   "source": [
    "def error_rate(xtrain, ytrain, x, opts):\n",
    "    k     = opts['k']\n",
    "    fold  = opts['fold']\n",
    "    xt    = fold['xt']\n",
    "    yt    = fold['yt']\n",
    "    xv    = fold['xv']\n",
    "    yv    = fold['yv']\n",
    "    \n",
    "    num_train = np.size(xt, 0)\n",
    "    num_valid = np.size(xv, 0)\n",
    "    xtrain  = xt[:, x == 1]\n",
    "    ytrain  = yt.reshape(num_train)  \n",
    "    xvalid  = xv[:, x == 1]\n",
    "    yvalid  = yv.reshape(num_valid)     \n",
    "\n",
    "    mdl     = KNeighborsClassifier(n_neighbors = k)\n",
    "    mdl.fit(xtrain, ytrain)\n",
    "    ypred   = mdl.predict(xvalid)\n",
    "    acc     = np.sum(yvalid == ypred) / num_valid\n",
    "    error   = 1 - acc\n",
    "    \n",
    "    return error\n",
    "\n",
    "\n",
    "def Fun(xtrain, ytrain, x, opts):\n",
    "    alpha    = 0.99\n",
    "    beta     = 1 - alpha\n",
    "    max_feat = len(x)\n",
    "    num_feat = np.sum(x == 1)\n",
    "    if num_feat == 0:\n",
    "        cost  = 1\n",
    "    else:\n",
    "        error = error_rate(xtrain, ytrain, x, opts)\n",
    "        cost  = alpha * error + beta * (num_feat / max_feat)\n",
    "        \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N7SLh_ApTf_7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "\n",
    "def init_position(lb, ub, N, dim):\n",
    "    X = np.zeros([N, dim], dtype='float')\n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def binary_conversion(X, thres, N, dim):\n",
    "    Xbin = np.zeros([N, dim], dtype='int')\n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            if X[i,d] > thres:\n",
    "                Xbin[i,d] = 1\n",
    "            else:\n",
    "                Xbin[i,d] = 0\n",
    "    \n",
    "    return Xbin\n",
    "\n",
    "\n",
    "def roulette_wheel(prob):\n",
    "    num = len(prob)\n",
    "    C   = np.cumsum(prob)\n",
    "    P   = rand()\n",
    "    for i in range(num):\n",
    "        if C[i] > P:\n",
    "            index = i;\n",
    "            break\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def jfs(xtrain, ytrain, opts):\n",
    "    ub       = 1\n",
    "    lb       = 0\n",
    "    thres    = 0.5    \n",
    "    CR       = 0.8     \n",
    "    MR       = 0.01   \n",
    "    N        = opts['N']\n",
    "    max_iter = opts['T']\n",
    "    if 'CR' in opts:\n",
    "        CR   = opts['CR'] \n",
    "    if 'MR' in opts: \n",
    "        MR   = opts['MR']  \n",
    " \n",
    "    dim = np.size(xtrain, 1)\n",
    "    if np.size(lb) == 1:\n",
    "        ub = ub * np.ones([1, dim], dtype='float')\n",
    "        lb = lb * np.ones([1, dim], dtype='float')\n",
    "        \n",
    "    X     = init_position(lb, ub, N, dim)\n",
    "    \n",
    "    X     = binary_conversion(X, thres, N, dim)\n",
    "    \n",
    "    fit   = np.zeros([N, 1], dtype='float')\n",
    "    Xgb   = np.zeros([1, dim], dtype='int')\n",
    "    fitG  = float('inf')\n",
    "    \n",
    "    for i in range(N):\n",
    "        fit[i,0] = Fun(xtrain, ytrain, X[i,:], opts)\n",
    "        if fit[i,0] < fitG:\n",
    "            Xgb[0,:] = X[i,:]\n",
    "            fitG     = fit[i,0]\n",
    "    \n",
    "    curve = np.zeros([1, max_iter], dtype='float')\n",
    "    t     = 0\n",
    "    \n",
    "    curve[0,t] = fitG.copy()\n",
    "    print(\"Generation:\", t + 1)\n",
    "    print(\"Best (GA):\", curve[0,t])\n",
    "    t += 1\n",
    "    \n",
    "    while t < max_iter:\n",
    "        inv_fit = 1 / (1 + fit)\n",
    "        prob    = inv_fit / np.sum(inv_fit) \n",
    " \n",
    "        Nc = 0\n",
    "        for i in range(N):\n",
    "            if rand() < CR:\n",
    "                Nc += 1\n",
    "              \n",
    "        x1 = np.zeros([Nc, dim], dtype='int')\n",
    "        x2 = np.zeros([Nc, dim], dtype='int')\n",
    "        for i in range(Nc):\n",
    "            k1      = roulette_wheel(prob)\n",
    "            k2      = roulette_wheel(prob)\n",
    "            P1      = X[k1,:].copy()\n",
    "            P2      = X[k2,:].copy()\n",
    "            index   = np.random.randint(low = 1, high = dim-1)\n",
    "            x1[i,:] = np.concatenate((P1[0:index] , P2[index:]))\n",
    "            x2[i,:] = np.concatenate((P2[0:index] , P1[index:]))\n",
    "            for d in range(dim):\n",
    "                if rand() < MR:\n",
    "                    x1[i,d] = 1 - x1[i,d]\n",
    "                    \n",
    "                if rand() < MR:\n",
    "                    x2[i,d] = 1 - x2[i,d]\n",
    "            Xnew = np.concatenate((x1 , x2), axis=0)\n",
    "        \n",
    "    \n",
    "        Fnew = np.zeros([2 * Nc, 1], dtype='float')\n",
    "        for i in range(2 * Nc):\n",
    "            Fnew[i,0] = Fun(xtrain, ytrain, Xnew[i,:], opts)\n",
    "            if Fnew[i,0] < fitG:\n",
    "                Xgb[0,:] = Xnew[i,:]\n",
    "                fitG     = Fnew[i,0]\n",
    "                   \n",
    "        curve[0,t] = fitG.copy()\n",
    "        print(\"Generation:\", t + 1)\n",
    "        print(\"Best (GA):\", curve[0,t])\n",
    "        t += 1\n",
    "        \n",
    "        XX  = np.concatenate((X , Xnew), axis=0)\n",
    "        FF  = np.concatenate((fit , Fnew), axis=0)\n",
    "        ind = np.argsort(FF, axis=0)\n",
    "        for i in range(N):\n",
    "            X[i,:]   = XX[ind[i,0],:]\n",
    "            fit[i,0] = FF[ind[i,0]]\n",
    "       \n",
    "            \n",
    "    Gbin       = Xgb[0,:]\n",
    "    Gbin       = Gbin.reshape(dim)\n",
    "    pos        = np.asarray(range(0, dim))    \n",
    "    sel_index  = pos[Gbin == 1]\n",
    "    num_feat   = len(sel_index)\n",
    "    ga_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
    "    \n",
    "    return ga_data \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the genetic algorithm to view the features selected and to see fitness vs iterations graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UQdF7YWYSD9U",
    "outputId": "ecd55e01-eb50-4a1b-af91-c724ed061e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '1' '1' '1' '1' '1' '1' '1'\n",
      " '1' '1' '1' '1' '1' '0' '0' '1' '0' '0' '1' '0']\n",
      "Generation: 1\n",
      "Best (GA): 0.22516666666666665\n",
      "Generation: 2\n",
      "Best (GA): 0.22449999999999998\n",
      "Generation: 3\n",
      "Best (GA): 0.11433333333333338\n",
      "Generation: 4\n",
      "Best (GA): 0.11433333333333338\n",
      "Generation: 5\n",
      "Best (GA): 0.11433333333333338\n",
      "Generation: 6\n",
      "Best (GA): 0.005833333333333339\n",
      "Generation: 7\n",
      "Best (GA): 0.005833333333333339\n",
      "Generation: 8\n",
      "Best (GA): 0.005666666666666671\n",
      "Generation: 9\n",
      "Best (GA): 0.005500000000000006\n",
      "Generation: 10\n",
      "Best (GA): 0.005333333333333338\n",
      "Generation: 11\n",
      "Best (GA): 0.005333333333333338\n",
      "Generation: 12\n",
      "Best (GA): 0.005333333333333338\n",
      "Generation: 13\n",
      "Best (GA): 0.005166666666666672\n",
      "Generation: 14\n",
      "Best (GA): 0.005166666666666672\n",
      "Generation: 15\n",
      "Best (GA): 0.0050000000000000044\n",
      "Generation: 16\n",
      "Best (GA): 0.0050000000000000044\n",
      "Generation: 17\n",
      "Best (GA): 0.0050000000000000044\n",
      "Generation: 18\n",
      "Best (GA): 0.004833333333333338\n",
      "Generation: 19\n",
      "Best (GA): 0.0046666666666666705\n",
      "Generation: 20\n",
      "Best (GA): 0.0046666666666666705\n",
      "Generation: 21\n",
      "Best (GA): 0.0046666666666666705\n",
      "Generation: 22\n",
      "Best (GA): 0.0046666666666666705\n",
      "Generation: 23\n",
      "Best (GA): 0.004500000000000004\n",
      "Generation: 24\n",
      "Best (GA): 0.0043333333333333375\n",
      "Generation: 25\n",
      "Best (GA): 0.0043333333333333375\n",
      "Generation: 26\n",
      "Best (GA): 0.0043333333333333375\n",
      "Generation: 27\n",
      "Best (GA): 0.004166666666666671\n",
      "Generation: 28\n",
      "Best (GA): 0.004166666666666671\n",
      "Generation: 29\n",
      "Best (GA): 0.0040000000000000036\n",
      "Generation: 30\n",
      "Best (GA): 0.0040000000000000036\n",
      "Generation: 31\n",
      "Best (GA): 0.0040000000000000036\n",
      "Generation: 32\n",
      "Best (GA): 0.003833333333333337\n",
      "Generation: 33\n",
      "Best (GA): 0.003833333333333337\n",
      "Generation: 34\n",
      "Best (GA): 0.003833333333333337\n",
      "Generation: 35\n",
      "Best (GA): 0.003833333333333337\n",
      "Generation: 36\n",
      "Best (GA): 0.0036666666666666696\n",
      "Generation: 37\n",
      "Best (GA): 0.0036666666666666696\n",
      "Generation: 38\n",
      "Best (GA): 0.0036666666666666696\n",
      "Generation: 39\n",
      "Best (GA): 0.0036666666666666696\n",
      "Generation: 40\n",
      "Best (GA): 0.0035000000000000027\n",
      "Generation: 41\n",
      "Best (GA): 0.003333333333333336\n",
      "Generation: 42\n",
      "Best (GA): 0.003166666666666669\n",
      "Generation: 43\n",
      "Best (GA): 0.003166666666666669\n",
      "Generation: 44\n",
      "Best (GA): 0.0030000000000000027\n",
      "Generation: 45\n",
      "Best (GA): 0.0030000000000000027\n",
      "Generation: 46\n",
      "Best (GA): 0.0030000000000000027\n",
      "Generation: 47\n",
      "Best (GA): 0.0028333333333333357\n",
      "Generation: 48\n",
      "Best (GA): 0.0028333333333333357\n",
      "Generation: 49\n",
      "Best (GA): 0.002666666666666669\n",
      "Generation: 50\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 51\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 52\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 53\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 54\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 55\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 56\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 57\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 58\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 59\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 60\n",
      "Best (GA): 0.0023333333333333353\n",
      "Generation: 61\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 62\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 63\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 64\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 65\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 66\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 67\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 68\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 69\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 70\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 71\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 72\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 73\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 74\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 75\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 76\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 77\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 78\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 79\n",
      "Best (GA): 0.0021666666666666687\n",
      "Generation: 80\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 81\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 82\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 83\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 84\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 85\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 86\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 87\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 88\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 89\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 90\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 91\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 92\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 93\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 94\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 95\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 96\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 97\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 98\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 99\n",
      "Best (GA): 0.0020000000000000018\n",
      "Generation: 100\n",
      "Best (GA): 0.0020000000000000018\n",
      "[ 0  2  5  7  8 12 34 40 42 43 46 54]\n",
      "Accuracy: 100.0\n",
      "Feature Size: 12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data  = df\n",
    "data  = data.values\n",
    "feat  = np.asarray(data[:, 0:-1])\n",
    "label = np.asarray(data[:, -1])\n",
    "print(label)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0.3, stratify=label)\n",
    "fold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}\n",
    "\n",
    "\n",
    "k    = 5   \n",
    "N    = 10   \n",
    "T    = 100   \n",
    "CR   = 0.8\n",
    "MR   = 0.01\n",
    "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'CR':CR, 'MR':MR}\n",
    "\n",
    "fmdl = jfs(feat, label, opts)\n",
    "sf   = fmdl['sf']\n",
    "print(sf)\n",
    "\n",
    "num_train = np.size(xtrain, 0)\n",
    "num_valid = np.size(xtest, 0)\n",
    "x_train   = xtrain[:, sf]\n",
    "y_train   = ytrain.reshape(num_train)  \n",
    "x_valid   = xtest[:, sf]\n",
    "y_valid   = ytest.reshape(num_valid)  \n",
    "\n",
    "mdl       = KNeighborsClassifier(n_neighbors = k) \n",
    "mdl.fit(x_train, y_train)\n",
    "\n",
    "y_pred    = mdl.predict(x_valid)\n",
    "Acc       = np.sum(y_valid == y_pred)  / num_valid\n",
    "print(\"Accuracy:\", 100 * Acc)\n",
    "\n",
    "num_feat = fmdl['nf']\n",
    "print(\"Feature Size:\", num_feat)\n",
    "\n",
    "curve   = fmdl['c']\n",
    "curve   = curve.reshape(np.size(curve,1))\n",
    "x       = np.arange(0, opts['T'], 1.0) + 1.0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, curve, 'o-')\n",
    "ax.set_xlabel('Number of Iterations')\n",
    "ax.set_ylabel('Fitness')\n",
    "ax.set_title('GA')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen features are : [ 0  2  5  7  8 12 34 40 42 43 46 54]. These are moved to a seperate dataset to perform the final classification using SVM"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Genetic Algorithm Feat Selection Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
